from typing import Any, Dict, List
import json

from logging_config import logger
from rag.retriever import get_rag_client_by_provider
from rag.pdf_loader import PDFLoader, LoadStatus
from settings import settings
from models import get_llm_by_usage
from prompts.template import apply_prompt_template
from langchain.tools import tool
from langchain.agents import create_agent

# Global RAG client and PDF loader for tools
_rag_client = None
_pdf_loader = None

def _get_rag_client():
    global _rag_client
    if _rag_client is None:
        _rag_client = get_rag_client_by_provider(settings.rag_provider)
    return _rag_client

def _get_pdf_loader():
    global _pdf_loader
    if _pdf_loader is None:
        if settings.chunk_strategy == "contextual":
            logger.info("Initializing PDFLoader with contextual chunking strategy.")
            _pdf_loader = PDFLoader(_get_rag_client(), llm_client=get_llm_by_usage('contextual'))
        else:
            _pdf_loader = PDFLoader(_get_rag_client())
    return _pdf_loader

# Global LLM for agentic tools
_agentic_llm = None

def _get_agentic_llm():
    global _agentic_llm
    if _agentic_llm is None:
        _agentic_llm = get_llm_by_usage('agentic')
    return _agentic_llm


# ============== Phase 1: Agentic Retrieval Tools ==============

@tool
def analyze_query(query: str) -> str:
    """
    [Phase 1] åˆ†æžç”¨æˆ·æŸ¥è¯¢ï¼Œç”Ÿæˆæ£€ç´¢ç­–ç•¥å’Œå¤šä¸ªå­æŸ¥è¯¢ã€‚
    è¿™æ˜¯ Agentic Retrieval çš„ç¬¬ä¸€æ­¥ï¼Œå¿…é¡»åœ¨æ£€ç´¢å‰è°ƒç”¨ï¼
    
    Args:
        query: ç”¨æˆ·çš„åŽŸå§‹æŸ¥è¯¢
    
    Returns:
        JSONæ ¼å¼çš„åˆ†æžç»“æžœï¼ŒåŒ…å«ï¼š
        - query_type: æŸ¥è¯¢ç±»åž‹ï¼ˆcomparison/definition/survey/technical_detailï¼‰
        - key_concepts: å…³é”®æ¦‚å¿µåˆ—è¡¨
        - sub_queries: å­æŸ¥è¯¢åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æŽ’åºï¼‰
        - estimated_complexity: å¤æ‚åº¦ï¼ˆhigh/medium/lowï¼‰
        - should_use_hyde: æ˜¯å¦åº”è¯¥ä½¿ç”¨ HyDE
    """
    logger.info("="*80)
    logger.info("ðŸŽ¯ [PHASE 1: QUERY ANALYSIS] Starting query analysis...")
    logger.info(f"ðŸ“ Original Query: {query}")
    logger.info("="*80)
    
    llm = _get_agentic_llm()
    
    prompt = f"""You are a research query analyzer. Analyze the following user query and generate a retrieval strategy.

User Query: "{query}"

Your task:
1. Identify the query type (comparison, definition, survey, technical_detail, or other)
2. Extract key concepts and terminology
3. Generate 2-4 sub-queries that progressively explore different aspects
   - Start with broad/general queries
   - Progress to specific/detailed queries
4. Estimate query complexity (high/medium/low)
5. Decide if HyDE (hypothetical document generation) would help

Guidelines for sub-queries:
- For comparisons: separate queries for each entity, then comparison
- For surveys: broad overview first, then specific techniques/methods
- For technical details: background first, then specific mechanisms
- Each sub-query should be self-contained and searchable

Respond ONLY with a valid JSON object (no markdown, no explanations):
{{
  "query_type": "comparison|definition|survey|technical_detail|other",
  "key_concepts": ["concept1", "concept2"],
  "sub_queries": ["query1", "query2", "query3"],
  "estimated_complexity": "high|medium|low",
  "should_use_hyde": true|false,
  "reasoning": "brief explanation of strategy"
}}"""
    
    try:
        response = llm.invoke(prompt)
        content = response.content if hasattr(response, 'content') else str(response)
        
        # Extract JSON from response (handle markdown code blocks)
        content = content.strip()
        if content.startswith('```'):
            # Remove markdown code block markers
            lines = content.split('\n')
            content = '\n'.join(lines[1:-1]) if len(lines) > 2 else content
            content = content.replace('```json', '').replace('```', '').strip()
        
        # Validate JSON
        analysis = json.loads(content)
        
        logger.success("âœ… Query analysis completed successfully!")
        logger.info(f"   ðŸ“Š Query Type: {analysis.get('query_type')}")
        logger.info(f"   ðŸ”¥ Complexity: {analysis.get('estimated_complexity')}")
        logger.info(f"   ðŸ”‘ Key Concepts: {', '.join(analysis.get('key_concepts', []))}")
        logger.info(f"   ðŸ“‹ Generated {len(analysis.get('sub_queries', []))} sub-queries:")
        for i, sq in enumerate(analysis.get('sub_queries', []), 1):
            logger.info(f"      {i}. {sq}")
        logger.info(f"   ðŸš€ Use HyDE: {analysis.get('should_use_hyde')}")
        logger.info(f"   ðŸ’¡ Reasoning: {analysis.get('reasoning', 'N/A')}")
        logger.info("="*80)
        
        return json.dumps(analysis, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"âŒ Query analysis failed: {e}")
        # Fallback: return simple analysis
        fallback = {
            "query_type": "other",
            "key_concepts": [query],
            "sub_queries": [query],
            "estimated_complexity": "medium",
            "should_use_hyde": False,
            "reasoning": f"Analysis failed, using original query. Error: {str(e)}"
        }
        return json.dumps(fallback, ensure_ascii=False, indent=2)


@tool
def generate_hypothetical_answer(query: str) -> str:
    """
    [Optional - HyDE] ç”Ÿæˆå‡æƒ³çš„ç†æƒ³ç­”æ¡ˆæ–‡æ¡£ï¼Œç”¨äºŽæ”¹å–„æ£€ç´¢è´¨é‡ã€‚
    é€‚ç”¨äºŽæŠ½è±¡/é«˜å±‚æ¬¡çš„æŸ¥è¯¢ã€‚ç”Ÿæˆçš„æ–‡æ¡£ä¼šè¢«ç”¨äºŽå‘é‡æ£€ç´¢ã€‚
    
    Args:
        query: å­æŸ¥è¯¢æˆ–åŽŸå§‹æŸ¥è¯¢
    
    Returns:
        å‡æƒ³çš„ç­”æ¡ˆæ–‡æ¡£æ–‡æœ¬ï¼ˆä¼šè¢« embedding åŽç”¨äºŽæ£€ç´¢ï¼‰
    """
    logger.info("="*80)
    logger.info("ðŸ”® [HyDE] Generating hypothetical answer document...")
    logger.info(f"ðŸ“ Query: {query}")
    logger.info("="*80)
    
    llm = _get_agentic_llm()
    
    prompt = f"""You are an expert researcher. Generate a hypothetical answer to the following query.

Query: "{query}"

Write a detailed, well-structured answer (2-3 paragraphs) as if you were writing an abstract or introduction section of a research paper that perfectly answers this query.

Include:
- Key technical terms and concepts
- Relevant methodologies or approaches
- Expected findings or conclusions
- References to common techniques or frameworks

Do NOT include citations like [1] or [2]. Just write the content.

Your hypothetical answer:"""
    
    try:
        response = llm.invoke(prompt)
        content = response.content if hasattr(response, 'content') else str(response)
        
        logger.success(f"âœ… Generated hypothetical document ({len(content)} chars)")
        logger.info(f"ðŸ“„ Preview: {content[:200]}...")
        logger.info("="*80)
        return content.strip()
    except Exception as e:
        logger.error(f"âŒ HyDE generation failed: {e}")
        logger.warning("âš ï¸  Falling back to original query")
        return query  # Fallback to original query


@tool
def evaluate_retrieval_progress(original_query: str, current_results_summary: str, round_number: int) -> str:
    """
    [Self-Reflection] è¯„ä¼°å½“å‰æ£€ç´¢ç»“æžœæ˜¯å¦å……åˆ†ï¼Œå†³å®šæ˜¯å¦éœ€è¦ç»§ç»­æ£€ç´¢ã€‚
    
    Args:
        original_query: ç”¨æˆ·çš„åŽŸå§‹æŸ¥è¯¢
        current_results_summary: å½“å‰å·²æ£€ç´¢ç»“æžœçš„æ‘˜è¦ï¼ˆè®ºæ–‡æ ‡é¢˜åˆ—è¡¨ï¼‰
        round_number: å½“å‰æ˜¯ç¬¬å‡ è½®æ£€ç´¢ï¼ˆ1-basedï¼‰
    
    Returns:
        JSONæ ¼å¼çš„è¯„ä¼°ç»“æžœï¼ŒåŒ…å«ï¼š
        - is_sufficient: æ˜¯å¦å·²å……åˆ†
        - coverage_score: è¦†ç›–åº¦è¯„åˆ† (0.0-1.0)
        - missing_aspects: ç¼ºå¤±çš„æ–¹é¢
        - should_continue: æ˜¯å¦åº”è¯¥ç»§ç»­æ£€ç´¢
        - next_focus: ä¸‹ä¸€æ­¥åº”è¯¥å…³æ³¨ä»€ä¹ˆ
    """
    logger.info("="*80)
    logger.info(f"ðŸ” [SELF-REFLECTION] Evaluating retrieval progress - Round {round_number}")
    logger.info(f"ðŸ“ Original Query: {original_query}")
    logger.info(f"ðŸ“Š Current Results Summary:")
    logger.info(current_results_summary[:500] + "..." if len(current_results_summary) > 500 else current_results_summary)
    logger.info("="*80)
    
    llm = _get_agentic_llm()
    
    prompt = f"""You are evaluating the sufficiency of retrieved research papers.

Original Query: "{original_query}"

Current Round: {round_number}

Retrieved Papers So Far:
{current_results_summary}

Your task:
1. Assess if the retrieved papers adequately cover the query
2. Identify any missing aspects or gaps
3. Decide if more retrieval rounds are needed
4. If continuing, suggest what to focus on next

Guidelines:
- Round 1-2: Usually continue unless results are perfect
- Round 3+: Only continue if critical information is missing
- Max 4 rounds recommended to avoid diminishing returns

Respond ONLY with a valid JSON object (no markdown, no explanations):
{{
  "is_sufficient": true|false,
  "coverage_score": 0.0-1.0,
  "missing_aspects": ["aspect1", "aspect2"],
  "should_continue": true|false,
  "next_focus": "description of what to search next",
  "reasoning": "brief explanation"
}}"""
    
    try:
        response = llm.invoke(prompt)
        content = response.content if hasattr(response, 'content') else str(response)
        
        # Extract JSON
        content = content.strip()
        if content.startswith('```'):
            lines = content.split('\n')
            content = '\n'.join(lines[1:-1]) if len(lines) > 2 else content
            content = content.replace('```json', '').replace('```', '').strip()
        
        evaluation = json.loads(content)
        
        logger.success(f"âœ… Evaluation completed - Round {round_number}")
        logger.info(f"   ðŸ“Š Coverage Score: {evaluation.get('coverage_score'):.2f}/1.0")
        logger.info(f"   âœ”ï¸  Is Sufficient: {evaluation.get('is_sufficient')}")
        logger.info(f"   âž¡ï¸  Should Continue: {evaluation.get('should_continue')}")
        if evaluation.get('missing_aspects'):
            logger.warning(f"   âš ï¸  Missing Aspects: {', '.join(evaluation.get('missing_aspects', []))}")
        if evaluation.get('next_focus'):
            logger.info(f"   ðŸŽ¯ Next Focus: {evaluation.get('next_focus')}")
        logger.info(f"   ðŸ’­ Reasoning: {evaluation.get('reasoning', 'N/A')}")
        logger.info("="*80)
        
        return json.dumps(evaluation, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"âŒ Evaluation failed: {e}")
        # Fallback: stop after round 3
        fallback = {
            "is_sufficient": round_number >= 3,
            "coverage_score": 0.5,
            "missing_aspects": [],
            "should_continue": round_number < 3,
            "next_focus": "Continue with remaining sub-queries",
            "reasoning": f"Evaluation failed, using heuristic. Error: {str(e)}"
        }
        return json.dumps(fallback, ensure_ascii=False, indent=2)


@tool
def rerank_results(original_query: str, results_json: str) -> str:
    """
    [Final Step] ä½¿ç”¨ LLM å¯¹æ£€ç´¢ç»“æžœè¿›è¡Œç›¸å…³æ€§è¯„åˆ†å’Œé‡æŽ’åºã€‚
    
    Args:
        original_query: ç”¨æˆ·çš„åŽŸå§‹æŸ¥è¯¢
        results_json: æ£€ç´¢ç»“æžœçš„ JSON å­—ç¬¦ä¸²ï¼ˆåŒ…å« title, abstract, doc_idï¼‰
    
    Returns:
        é‡æŽ’åºåŽçš„ç»“æžœï¼ˆJSON æ ¼å¼ï¼‰ï¼Œæ¯ä¸ªç»“æžœåŒ…å«ç›¸å…³æ€§åˆ†æ•°
    """
    logger.info("="*80)
    logger.info("ðŸ† [PHASE 3: RERANKING] Starting LLM-based reranking...")
    logger.info(f"ðŸ“ Query: {original_query}")
    
    llm = _get_agentic_llm()
    
    try:
        results = json.loads(results_json)
    except:
        logger.error("âŒ Failed to parse results JSON")
        return results_json  # Return as-is if parsing fails
    
    if not results:
        logger.warning("âš ï¸  No results to rerank")
        return results_json
    
    logger.info(f"ðŸ“Š Input: {len(results)} papers to rerank")
    logger.info("="*80)
    
    # Prepare results for LLM
    results_for_llm = []
    for i, r in enumerate(results[:15], 1):  # Limit to top 15 for efficiency
        results_for_llm.append({
            "index": i,
            "title": r.get("title", "Untitled"),
            "abstract": r.get("abstract", "")[:400],  # Truncate for token efficiency
            "doc_id": r.get("doc_id", "")
        })
    
    prompt = f"""You are a research paper relevance evaluator. Rate the relevance of each paper to the query.

Query: "{original_query}"

Papers:
{json.dumps(results_for_llm, ensure_ascii=False, indent=2)}

Your task:
For each paper, assign a relevance score from 0-10:
- 9-10: Highly relevant, directly addresses the query
- 7-8: Relevant, covers important aspects
- 5-6: Somewhat relevant, tangentially related
- 3-4: Marginally relevant
- 0-2: Not relevant or off-topic

Respond ONLY with a valid JSON array (no markdown, no explanations):
[
  {{"index": 1, "score": 8.5, "reason": "brief explanation"}},
  {{"index": 2, "score": 7.0, "reason": "brief explanation"}},
  ...
]"""
    
    try:
        response = llm.invoke(prompt)
        content = response.content if hasattr(response, 'content') else str(response)
        
        # Extract JSON
        content = content.strip()
        if content.startswith('```'):
            lines = content.split('\n')
            content = '\n'.join(lines[1:-1]) if len(lines) > 2 else content
            content = content.replace('```json', '').replace('```', '').strip()
        
        scores = json.loads(content)
        
        # Apply scores to results
        score_map = {s["index"]: s["score"] for s in scores if "index" in s and "score" in s}
        
        for i, r in enumerate(results[:15], 1):
            if i in score_map:
                r["llm_relevance_score"] = score_map[i]
            else:
                r["llm_relevance_score"] = 5.0  # Default middle score
        
        # Sort by LLM score
        reranked = sorted(results[:15], key=lambda x: x.get("llm_relevance_score", 0), reverse=True)
        
        # Filter out low scores (< 4.0)
        reranked = [r for r in reranked if r.get("llm_relevance_score", 0) >= 4.0]
        
        logger.success(f"âœ… Reranking completed!")
        logger.info(f"   ðŸ“Š Final Results: {len(reranked)} papers (filtered from {len(results)})")
        logger.info(f"   ðŸ† Top 5 Papers by Relevance:")
        for i, r in enumerate(reranked[:5], 1):
            score = r.get("llm_relevance_score", 0)
            title = r.get("title", "Untitled")[:60]
            logger.info(f"      {i}. [{score:.1f}/10] {title}...")
        logger.info("="*80)
        
        return json.dumps(reranked, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"âŒ Reranking failed: {e}")
        logger.warning("âš ï¸  Returning original results without reranking")
        return results_json  # Return original results


# ============== Phase 2: Abstract Search ==============

@tool
def search_abstracts(query: str, k: int = 5) -> str:
    """
    [Phase 2] æœç´¢è®ºæ–‡æ‘˜è¦ï¼Œæ‰¾å‡ºç›¸å…³è®ºæ–‡ã€‚
    è¿™æ˜¯æœç´¢çš„ç¬¬ä¸€æ­¥ï¼Œè¿”å›žå€™é€‰è®ºæ–‡åˆ—è¡¨ã€‚
    
    Args:
        query: æœç´¢å…³é”®è¯æˆ–è‡ªç„¶è¯­è¨€æŸ¥è¯¢
        k: è¿”å›žè®ºæ–‡æ•°é‡ (é»˜è®¤ 5)
    
    Returns:
        å€™é€‰è®ºæ–‡åˆ—è¡¨ï¼ŒåŒ…å« title, abstract é¢„è§ˆ, doc_id
    """
    logger.info("="*80)
    logger.info("ðŸ”Ž [PHASE 2: RETRIEVAL] Searching abstracts...")
    logger.info(f"ðŸ“ Query: {query}")
    logger.info(f"ðŸ“Š Requested: top {k} papers")
    logger.info("="*80)
    
    client = _get_rag_client()
    results = client.search_abstracts(query, k)
    
    if not results:
        logger.warning("âš ï¸  No papers found matching the query")
        return "No papers found matching the query."
    
    logger.success(f"âœ… Found {len(results)} papers")
    logger.info("ðŸ“„ Top 3 Results:")
    for i, r in enumerate(results[:3], 1):
        title = r.get('title', 'Untitled')[:60]
        logger.info(f"   {i}. {title}... (doc_id: {r.get('doc_id', 'N/A')[:8]}...)")
    logger.info("="*80)
    
    output = []
    for i, r in enumerate(results, 1):
        abstract = r.get("abstract", "")[:300] + "..." if len(r.get("abstract", "")) > 300 else r.get("abstract", "")
        output.append(
            f"[{i}] {r.get('title', 'Untitled')}\n"
            f"    doc_id: {r.get('doc_id', 'N/A')}\n"
            f"    Abstract: {abstract}"
        )
    
    return "\n\n".join(output)


# ============== Phase 3: Lazy Load PDF ==============

@tool
def load_paper_pdfs(doc_ids: List[str]) -> str:
    """
    [Phase 3] åŠ è½½æŒ‡å®šè®ºæ–‡çš„ PDF å†…å®¹åˆ°æ•°æ®åº“ã€‚
    åœ¨ä½¿ç”¨ search_paper_content ä¹‹å‰å¿…é¡»è°ƒç”¨æ­¤å·¥å…·ï¼
    ä¼šè‡ªåŠ¨è·³è¿‡å·²åŠ è½½çš„è®ºæ–‡ã€‚
    
    Args:
        doc_ids: è¦åŠ è½½çš„è®ºæ–‡ doc_id åˆ—è¡¨ï¼ˆä»Ž search_abstracts èŽ·å–ï¼‰
    
    Returns:
        åŠ è½½çŠ¶æ€æŠ¥å‘Š
    
    æ³¨æ„ï¼š
        - ä¸€æ¬¡å»ºè®®åŠ è½½ 3-5 ç¯‡è®ºæ–‡ï¼Œé¿å…ç­‰å¾…è¿‡é•¿
        - åŠ è½½è¿‡ç¨‹éœ€è¦ä¸‹è½½å’Œè§£æž PDFï¼Œå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´
    """
    loader = _get_pdf_loader()
    results = loader.load_papers(doc_ids)
    
    # æ ¼å¼åŒ–è¾“å‡º
    output = ["PDF Loading Results:"]
    success_count = 0
    skip_count = 0
    fail_count = 0
    
    for doc_id, result in results.items():
        status_icon = {
            LoadStatus.SUCCESS: "âœ“",
            LoadStatus.ALREADY_EXISTS: "â—‹",
            LoadStatus.DOWNLOAD_FAILED: "âœ—",
            LoadStatus.PARSE_FAILED: "âœ—",
            LoadStatus.NO_PDF_URL: "âœ—",
            LoadStatus.NOT_FOUND: "âœ—",
        }.get(result.status, "?")
        
        output.append(f"  {status_icon} {doc_id}: {result.message}")
        
        if result.status == LoadStatus.SUCCESS:
            success_count += 1
        elif result.status == LoadStatus.ALREADY_EXISTS:
            skip_count += 1
        else:
            fail_count += 1
    
    output.append(f"\nSummary: {success_count} loaded, {skip_count} skipped, {fail_count} failed")
    
    if success_count + skip_count > 0:
        output.append("\nYou can now use search_paper_content to search within these papers.")
    
    return "\n".join(output)


# ============== Phase 4: Deep Search ==============

@tool  
def search_paper_content(query: str, doc_ids: List[str] = [], category: int = -1, k: int = 5) -> str:
    """
    [Phase 4] åœ¨å·²åŠ è½½çš„è®ºæ–‡ä¸­æœç´¢å…·ä½“å†…å®¹ã€‚
    æ³¨æ„ï¼šå¿…é¡»å…ˆç”¨ load_paper_pdfs åŠ è½½è®ºæ–‡ï¼
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        doc_ids: è¦æœç´¢çš„è®ºæ–‡ doc_id åˆ—è¡¨ï¼ˆç•™ç©ºåˆ™æœç´¢æ‰€æœ‰å·²åŠ è½½çš„è®ºæ–‡ï¼‰
        category: ç« èŠ‚ç±»åž‹è¿‡æ»¤ï¼Œ-1 è¡¨ç¤ºå…¨éƒ¨
            0 = Abstract (æ‘˜è¦)
            1 = Introduction (èƒŒæ™¯ã€åŠ¨æœº)
            2 = Method (æŠ€æœ¯ç»†èŠ‚ã€ç®—æ³•ã€æž¶æž„)
            3 = Evaluation (å®žéªŒã€ç»“æžœã€æ€§èƒ½æ•°æ®)
            4 = Conclusion (ç»“è®º)
            6 = Related Work (æ³¨æ„ï¼šæè¿°çš„æ˜¯å…¶ä»–è®ºæ–‡ï¼)
        k: è¿”å›žç»“æžœæ•°é‡ (é»˜è®¤ 5)
    
    Returns:
        åŒ¹é…çš„æ–‡æœ¬ç‰‡æ®µåŠå…¶å…ƒæ•°æ®
    """
    client = _get_rag_client()
    
    # å¦‚æžœæŒ‡å®šäº† doc_idsï¼Œéœ€è¦é€ä¸ªæœç´¢å¹¶åˆå¹¶ç»“æžœ
    # TODO: ä¼˜åŒ–ä¸ºæ‰¹é‡æœç´¢
    if doc_ids:
        all_results = []
        for doc_id in doc_ids:
            results = client.search_by_section(
                query, 
                doc_id=doc_id, 
                section_category=category if category >= 0 else None, 
                k=k
            )
            all_results.extend(results)
        # æŒ‰ç›¸å…³æ€§æŽ’åºï¼ˆå‡è®¾æœ‰ score å­—æ®µï¼‰
        all_results.sort(key=lambda x: x.get("score", 0), reverse=True)
        results = all_results[:k]
    else:
        results = client.search_by_section(
            query, 
            doc_id=None, 
            section_category=category if category >= 0 else None, 
            k=k
        )
    
    if not results:
        return "No matching content found. Make sure you have loaded the papers first using load_paper_pdfs."
    
    from parser.pdf_parser import SectionCategory
    
    output = []
    for i, r in enumerate(results, 1):
        cat_id = r.get("section_category", 0)
        try:
            cat_name = SectionCategory(cat_id).name
        except:
            cat_name = "UNKNOWN"
        
        text = r.get("text", "")[:500] + "..." if len(r.get("text", "")) > 500 else r.get("text", "")
        
        output.append(
            f"[{i}] doc_id: {r.get('doc_id', 'N/A')} | chunk_id: {r.get('chunk_id', 'N/A')}\n"
            f"    Section: {cat_name} | Parent: {r.get('parent_section', 'N/A')}\n"
            f"    Text: {text}"
        )
    
    return "\n\n".join(output)


# ============== Context Tools ==============

@tool
def get_context_window(doc_id: str, chunk_id: int, window: int = 1) -> str:
    """
    èŽ·å–æŒ‡å®š chunk å‘¨å›´çš„ä¸Šä¸‹æ–‡æ–‡æœ¬ã€‚
    å½“æ£€ç´¢åˆ°çš„ç‰‡æ®µä¸å®Œæ•´æˆ–è¢«æˆªæ–­æ—¶ä½¿ç”¨ã€‚
    
    Args:
        doc_id: è®ºæ–‡çš„ doc_id
        chunk_id: chunk_idï¼ˆä»Ž search_paper_content ç»“æžœèŽ·å–ï¼‰
        window: å‰åŽå„åŒ…å«å¤šå°‘ä¸ª chunk (é»˜è®¤ 1)
    
    Returns:
        æ‰©å±•çš„ä¸Šä¸‹æ–‡æ–‡æœ¬
    """
    client = _get_rag_client()
    context = client.get_context_window(doc_id, chunk_id, window)
    
    if not context:
        return "Could not retrieve context for this chunk."
    
    return f"[Context Window for doc_id={doc_id}, chunk_id={chunk_id}]\n\n{context}"


# ============== Searcher Class ==============

class Searcher:
    """RAG searcher that supports both simple and agentic modes.

    - Simple mode: Direct vector search (only abstracts)
    - Agentic mode: LLM agent with Lazy Load PDF workflow
    
    Lazy Load Workflow:
    1. search_abstracts - æ‰¾å€™é€‰è®ºæ–‡
    2. load_paper_pdfs - æŒ‰éœ€åŠ è½½ PDF
    3. search_paper_content - æœç´¢æ­£æ–‡
    4. get_context_window - èŽ·å–æ›´å¤šä¸Šä¸‹æ–‡
    """

    def __init__(self) -> None:
        self.rag_client = get_rag_client_by_provider(settings.rag_provider)
        self.top_k = settings.milvus_top_k
        
        if settings.enable_agentic_rag:
            self.llm = get_llm_by_usage('agentic')
            self._setup_agent()

    def _setup_agent(self):
        """Setup the LangChain agent with Agentic Retrieval tools."""
        self.tools = [
            # Phase 1: Agentic Retrieval
            analyze_query,
            generate_hypothetical_answer,
            evaluate_retrieval_progress,
            rerank_results,
            # Phase 2: Abstract search
            search_abstracts,
            # Phase 3: Lazy load PDF
            load_paper_pdfs,
            # Phase 4: Deep search
            search_paper_content,
            # Context tools
            get_context_window,
        ]
        
        # Load prompt as system message
        prompt_msgs = apply_prompt_template("agentic_searcher")
        self.system_prompt = prompt_msgs[0]["content"]

    def _agentic_search(self, query: str) -> Dict[str, Any]:
        """Run the agentic search loop."""
        logger.info("\n" + "ðŸš€"*40)
        logger.info("ðŸ¤– AGENTIC RAG PIPELINE STARTED")
        logger.info(f"ðŸ“ User Query: {query}")
        logger.info("ðŸš€"*40 + "\n")
        
        agent = create_agent(
            model=self.llm, 
            tools=self.tools,
        )
        
        # Build messages with system prompt and user query
        msgs = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": query}
        ]
        
        try:
            result = agent.invoke({"messages": msgs}, config={"recursion_limit": 100})
            messages = result.get("messages", [])
            # Get the last AI message as the answer
            answer = ""
            for msg in reversed(messages):
                if hasattr(msg, 'content') and msg.content:
                    answer = msg.content
                    break
            
            logger.info("\n" + "âœ…"*40)
            logger.info("ðŸŽ‰ AGENTIC RAG PIPELINE COMPLETED")
            logger.info(f"ðŸ“Š Total Messages: {len(messages)}")
            logger.info(f"ðŸ“ Answer Length: {len(answer)} chars")
            logger.info("âœ…"*40 + "\n")
            
            return {
                "answer": answer,
                "intermediate_steps": messages
            }
        except Exception as e:
            logger.error(f"âŒ Agentic search failed: {e}")
            logger.error("="*80)
            return {"answer": f"Search failed: {e}", "intermediate_steps": []}

    def search(self, query: str, k: int | None = None) -> List[Dict[str, Any]]:
        """Query vector store and return normalized hits with ids."""
        k = k or self.top_k
        
        if settings.enable_agentic_rag:
            # Agentic mode: return the agent's analysis
            result = self._agentic_search(query)
            # For compatibility, wrap the answer in a hit-like structure
            return [{
                "id": 1,
                "title": "Agentic Search Result",
                "abstract": result["answer"],
                "url": "",
                "doc_id": "agentic",
                "score": 1.0,
                "conference_name": "",
                "conference_year": "",
                "conference_round": "",
                "section_category": 0,
                "parent_section": "",
                "page_number": 0,
            }]
            
        # Simple mode: direct vector search
        raw_hits = self.rag_client.query_relevant_documents(query)
        hits: List[Dict[str, Any]] = []
        for idx, hit in enumerate(raw_hits[:k]):
            hits.append(
                {
                    "id": idx + 1,
                    "title": hit.get("title", ""),
                    "abstract": hit.get("abstract", ""),
                    "url": hit.get("url", ""),
                    "doc_id": hit.get("doc_id", ""),
                    "score": hit.get("score", 0.0),
                    "conference_name": hit.get("conference_name", ""),
                    "conference_year": hit.get("conference_year", ""),
                    "conference_round": hit.get("conference_round", ""),
                    "section_category": hit.get("section_category", 0),
                    "parent_section": hit.get("parent_section", ""),
                    "page_number": hit.get("page_number", 0),
                }
            )
        logger.info(f"Searcher retrieved {len(hits)} hits for query: {query}")
        return hits

    def format_hits(self, hits: List[Dict[str, Any]], max_len: int = 600) -> str:
        """Helper for coordinator: render hits into concise numbered blocks."""
        if not hits:
            return "No relevant documents found."
        
        # Check if this is an agentic result
        if len(hits) == 1 and hits[0].get("doc_id") == "agentic":
            return hits[0].get("abstract", "No answer generated.")
        
        from parser.pdf_parser import SectionCategory
        
        blocks = []
        for h in hits:
            meta_parts = []
            if h.get("conference_name"):
                meta_parts.append(str(h["conference_name"]))
            if h.get("conference_year"):
                meta_parts.append(str(h["conference_year"]))
            
            # Add structure info to metadata
            cat_id = h.get("section_category", 0)
            try:
                cat_name = SectionCategory(cat_id).name
            except:
                cat_name = "UNKNOWN"
            
            if cat_name != "ABSTRACT":
                meta_parts.append(f"Section: {cat_name}")
            
            parent = h.get("parent_section")
            if parent:
                meta_parts.append(f"Parent: {parent}")
                
            page = h.get("page_number")
            if page and page > 0:
                meta_parts.append(f"Page: {page}")

            meta = " | ".join(meta_parts)
            abstract = h.get("abstract", "")
            if len(abstract) > max_len:
                abstract = abstract[:max_len] + "..."
            blocks.append(
                f"[{h['id']}] {h.get('title') or 'Untitled'}\n"
                f"{meta + '\\n' if meta else ''}"
                f"URL: {h.get('url') or 'N/A'}\n"
                f"Content: {abstract}"
            )
        return "\n\n".join(blocks)


def invoke_searcher(query: str, k: int | None = None) -> List[Dict[str, Any]]:
    """Convenience wrapper used by coordinator/tools. Returns hits only."""
    searcher = Searcher()
    return searcher.search(query, k)
