# RAG è¯„ä¼°æ¡†æ¶æ–‡æ¡£

## ç›®å½•
1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [æ¶æ„æ¦‚è§ˆ](#æ¶æ„æ¦‚è§ˆ)
3. [è¯„ä¼°æŒ‡æ ‡è¯¦è§£](#è¯„ä¼°æŒ‡æ ‡è¯¦è§£)
4. [å¯¹æ¯”å®éªŒé…ç½®](#å¯¹æ¯”å®éªŒé…ç½®)
5. [æ•°æ®æµç¨‹](#æ•°æ®æµç¨‹)
6. [API å‚è€ƒ](#api-å‚è€ƒ)

---

## å¿«é€Ÿå¼€å§‹

### 1. è¿è¡Œè¯„ä¼°ï¼ˆä½¿ç”¨å·²æœ‰æ•°æ®ï¼‰

```bash
# L1 + L2 è¯„ä¼°ï¼ˆå¿«é€Ÿï¼Œçº¦ 5 ç§’ï¼‰
uv run python tests/test_evaluation_runner.py

# L1 + L2 + L3 è¯„ä¼°ï¼ˆéœ€è¦ LLMï¼Œçº¦ 20-30 åˆ†é’Ÿï¼‰
uv run python tests/test_evaluation_runner.py --full

# åªè¿è¡Œ L1 Paper Discovery
uv run python tests/test_evaluation_runner.py --l1-only

# åªè¿è¡Œ L3 End-to-End
uv run python tests/test_evaluation_runner.py --l3-only
```

### 2. æŸ¥çœ‹æ•°æ®çŠ¶æ€

```bash
uv run python scripts/run_full_evaluation.py --status
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
ğŸ“„ Source Papers: 821 ç¯‡
ğŸ“¦ Chunks: 8 ç¯‡è®ºæ–‡, 1486 ä¸ª chunks
â“ Ground Truth: 5 ä¸ª QA pairs
ğŸ—„ï¸ Eval Collection: papers_eval_paragraph (1494 æ¡è®°å½•)
ğŸ“Š Reports: 3 ä¸ªæŠ¥å‘Š
```

### 3. ç”Ÿæˆæ›´å¤šæµ‹è¯•æ•°æ®

```bash
# ä»ç°æœ‰ chunks ç”Ÿæˆæ›´å¤š QAï¼ˆä¸éœ€è¦é‡æ–°å¤„ç† PDFï¼‰
uv run python scripts/run_full_evaluation.py --generate-qa --num-questions 30

# ä»ä¸šåŠ¡åº“æŠ½æ ·æ›´å¤šè®ºæ–‡ï¼ˆéœ€è¦ä¸‹è½½ PDFï¼‰
uv run python scripts/run_full_evaluation.py --full --sample 20 --num-questions 30
```

---

## å®Œæ•´å¯¹æ¯”å®éªŒæµç¨‹

ä½¿ç”¨ `--compare` æ¨¡å¼å¯ä»¥ä¸€é”®è¿è¡Œ **12 ç§é…ç½®**çš„å¯¹æ¯”å®éªŒï¼š

```bash
uv run python scripts/run_full_evaluation.py --compare --sample 20 --num-questions 30
```

### æµç¨‹æ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           å®Œæ•´å¯¹æ¯”å®éªŒæµç¨‹                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  Step 1: æ•°æ®å‡†å¤‡                                                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                            â”‚
â”‚  â€¢ ä»ä¸šåŠ¡åº“ (papers_rag) æŠ½æ ·è®ºæ–‡å…ƒæ•°æ®                                        â”‚
â”‚  â€¢ ä¸‹è½½ PDF æ–‡ä»¶åˆ°æœ¬åœ°                                                        â”‚
â”‚  â€¢ ä½¿ç”¨ä¸¤ç§ç­–ç•¥åˆ†åˆ«å¤„ç† chunks:                                                â”‚
â”‚      - paragraph: ç›´æ¥æŒ‰æ®µè½åˆ†å—                                              â”‚
â”‚      - contextual: LLM å¢å¼ºä¸Šä¸‹æ–‡åˆ†å—                                         â”‚
â”‚  â€¢ åˆ›å»º 2 ä¸ªè¯„ä¼° collection:                                                  â”‚
â”‚      - papers_eval_paragraph                                                 â”‚
â”‚      - papers_eval_contextual                                                â”‚
â”‚                                                                              â”‚
â”‚  Step 2: QA ç”Ÿæˆ                                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                              â”‚
â”‚  â€¢ ä» chunks ä¸­ç”¨ LLM ç”Ÿæˆé—®ç­”å¯¹                                              â”‚
â”‚  â€¢ æ¯ä¸ª QA åŒ…å«: é—®é¢˜ã€ç­”æ¡ˆã€æ¥æºè®ºæ–‡ã€æ¥æºç« èŠ‚ã€æœŸæœ› chunks                      â”‚
â”‚  â€¢ ä¿å­˜åˆ° ground_truth.json                                                  â”‚
â”‚                                                                              â”‚
â”‚  Step 3: è¿è¡Œ 12 ç§å®éªŒ                                                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                       â”‚
â”‚  å¯¹äºæ¯ç§é…ç½®ç»„åˆ:                                                             â”‚
â”‚                                                                              â”‚
â”‚    2 Chunk ç­–ç•¥   Ã—   3 Index ç±»å‹   Ã—   2 RAG æ¨¡å¼   =   12 ç§é…ç½®           â”‚
â”‚    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚
â”‚    â€¢ paragraph       â€¢ FLAT (ç²¾ç¡®)      â€¢ basic                              â”‚
â”‚    â€¢ contextual      â€¢ HNSW (è¿‘ä¼¼)      â€¢ agentic                            â”‚
â”‚                      â€¢ IVF_FLAT                                              â”‚
â”‚                                                                              â”‚
â”‚  æ¯ç§é…ç½®æ‰§è¡Œ:                                                                 â”‚
â”‚    1. åˆ‡æ¢åˆ°å¯¹åº”çš„ collection                                                 â”‚
â”‚    2. é‡å»ºå¯¹åº”ç±»å‹çš„ç´¢å¼•                                                       â”‚
â”‚    3. è¿è¡Œ L1 è¯„ä¼° (Paper Discovery)                                          â”‚
â”‚    4. è¿è¡Œ L2 è¯„ä¼° (Section Retrieval)                                        â”‚
â”‚    5. è¿è¡Œ L3 è¯„ä¼° (End-to-End QA, å¯é€‰)                                      â”‚
â”‚    6. è®°å½•æŒ‡æ ‡                                                                â”‚
â”‚                                                                              â”‚
â”‚  Step 4: ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š                                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                        â”‚
â”‚  â€¢ æ±‡æ€»æ‰€æœ‰é…ç½®çš„æŒ‡æ ‡                                                          â”‚
â”‚  â€¢ ç”Ÿæˆ Markdown å¯¹æ¯”è¡¨æ ¼                                                     â”‚
â”‚  â€¢ ä¿å­˜ JSON è¯¦ç»†æŠ¥å‘Š                                                         â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### èµ„æºè¯´æ˜

| èµ„æº | æ•°é‡ | è¯´æ˜ |
|------|------|------|
| **Milvus Collections** | 2 ä¸ª | `papers_eval_paragraph`, `papers_eval_contextual` |
| **ç´¢å¼•é‡å»º** | 6 æ¬¡ | æ¯ä¸ª collection é‡å»º 3 ç§ç´¢å¼•ç±»å‹ |
| **è¯„ä¼°è¿è¡Œ** | 12 æ¬¡ | 2 chunk Ã— 3 index Ã— 2 agentic |
| **LLM è°ƒç”¨** | ~30Ã—12 æ¬¡ (L3) | æ¯ä¸ª QA éœ€è¦ LLM è¯„åˆ¤ |

### å¿«é€Ÿæ¨¡å¼

```bash
# è·³è¿‡ L3 è¯„ä¼°ï¼ˆå¿«å¾ˆå¤šï¼Œåªæ¯”è¾ƒæ£€ç´¢æ€§èƒ½ï¼‰
uv run python scripts/run_full_evaluation.py --compare --sample 20 --no-l3

# ä½¿ç”¨å·²æœ‰æ•°æ®ï¼ˆè·³è¿‡ Step 1 å’Œ Step 2ï¼‰
uv run python scripts/run_full_evaluation.py --compare --skip-data-prep
```

---

## æ¶æ„æ¦‚è§ˆ

### ç³»ç»Ÿæ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Evaluation Framework                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Data Preparation â”‚    â”‚  QA Generation  â”‚    â”‚   Runner     â”‚ â”‚
â”‚  â”‚                  â”‚    â”‚                 â”‚    â”‚              â”‚ â”‚
â”‚  â”‚ â€¢ DataExporter   â”‚â”€â”€â”€â–¶â”‚ â€¢ QAGenerator   â”‚â”€â”€â”€â–¶â”‚ â€¢ L1 Eval    â”‚ â”‚
â”‚  â”‚ â€¢ PDFLoader      â”‚    â”‚ â€¢ Prompts       â”‚    â”‚ â€¢ L2 Eval    â”‚ â”‚
â”‚  â”‚ â€¢ CollectionBuilderâ”‚  â”‚                 â”‚    â”‚ â€¢ L3 Eval    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â”‚                      â”‚                     â”‚         â”‚
â”‚           â–¼                      â–¼                     â–¼         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ evaluation/data/ â”‚    â”‚ ground_truth.jsonâ”‚   â”‚ reports/*.jsonâ”‚ â”‚
â”‚  â”‚ â€¢ chunks/        â”‚    â”‚                 â”‚    â”‚              â”‚ â”‚
â”‚  â”‚ â€¢ pdfs/          â”‚    â”‚                 â”‚    â”‚              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         RAG Components                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  MilvusProvider â”‚    â”‚   Embedding     â”‚    â”‚     LLM      â”‚ â”‚
â”‚  â”‚                 â”‚    â”‚                 â”‚    â”‚              â”‚ â”‚
â”‚  â”‚ â€¢ search_abstractsâ”‚  â”‚ â€¢ qwen3-embeddingâ”‚   â”‚ â€¢ qwen3:8b   â”‚ â”‚
â”‚  â”‚ â€¢ search_by_sectionâ”‚ â”‚ â€¢ dim=2560      â”‚    â”‚              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒæ¨¡å—

| æ¨¡å— | è·¯å¾„ | èŒè´£ |
|------|------|------|
| **EvaluationRunner** | `src/evaluation/runner.py` | æ‰§è¡Œ L1/L2/L3 è¯„ä¼°ï¼Œç”ŸæˆæŠ¥å‘Š |
| **QAGenerator** | `src/evaluation/qa_generation/qa_generator.py` | ä» chunks ç”Ÿæˆæµ‹è¯• QA pairs |
| **DataPreparationPipeline** | `src/evaluation/data_preparation/pipeline.py` | æ•°æ®å¯¼å‡ºã€PDFå¤„ç†ã€chunkä¿å­˜ |
| **CollectionBuilder** | `src/evaluation/data_preparation/collection_builder.py` | ç®¡ç†è¯„ä¼° collectionï¼Œæ”¯æŒç­–ç•¥åˆ‡æ¢ |
| **EvaluationConfig** | `src/evaluation/config.py` | é…ç½®ç®¡ç†ï¼ˆè·¯å¾„ã€ç­–ç•¥ã€å‚æ•°ï¼‰ |

---

## è¯„ä¼°æŒ‡æ ‡è¯¦è§£

### L1: Paper Discoveryï¼ˆè®ºæ–‡å‘ç°ï¼‰

æµ‹è¯•ç›®æ ‡ï¼šç»™å®šä¸€ä¸ªé—®é¢˜ï¼Œèƒ½å¦æ‰¾åˆ°ç›¸å…³çš„è®ºæ–‡ï¼Ÿ

**ä½¿ç”¨çš„ RAG æ–¹æ³•**: `search_abstracts(query, k=10)`

| æŒ‡æ ‡ | å…¬å¼ | è¯´æ˜ |
|------|------|------|
| **Precision@K** | $P@K = \frac{\|Retrieved_K \cap Relevant\|}{K}$ | è¿”å›çš„ K ä¸ªç»“æœä¸­æœ‰å¤šå°‘æ˜¯ç›¸å…³çš„ |
| **Recall@K** | $R@K = \frac{\|Retrieved_K \cap Relevant\|}{\|Relevant\|}$ | ç›¸å…³æ–‡æ¡£ä¸­æœ‰å¤šå°‘è¢«è¿”å› |
| **MRR** | $MRR = \frac{1}{N}\sum_{i=1}^{N}\frac{1}{rank_i}$ | ç¬¬ä¸€ä¸ªç›¸å…³ç»“æœçš„æ’åå€’æ•°çš„å¹³å‡å€¼ |
| **Hit Rate** | $HR = \frac{\|queries\ with\ hit\|}{\|queries\|}$ | è‡³å°‘è¿”å›ä¸€ä¸ªç›¸å…³ç»“æœçš„æŸ¥è¯¢æ¯”ä¾‹ |

**ä»£ç å®ç°**:
```python
# src/evaluation/runner.py

def run_l1_paper_discovery(self, qa_pairs: list[QAPair]) -> L1Result:
    for qa in valid_pairs:
        # æ£€ç´¢
        results = self.rag.search_abstracts(query=qa.question, k=10)
        retrieved_docs = [r["doc_id"] for r in results]
        expected_docs = set(qa.expected_doc_ids)
        
        # è®¡ç®—æŒ‡æ ‡
        p_at_5 = len(set(retrieved_docs[:5]) & expected_docs) / 5
        p_at_10 = len(set(retrieved_docs[:10]) & expected_docs) / 10
        r_at_k = len(set(retrieved_docs[:k]) & expected_docs) / len(expected_docs)
        
        # MRR: æ‰¾åˆ°ç¬¬ä¸€ä¸ªç›¸å…³æ–‡æ¡£çš„ä½ç½®
        for rank, doc_id in enumerate(retrieved_docs, 1):
            if doc_id in expected_docs:
                mrr = 1.0 / rank
                break
```

---

### L2: Section Retrievalï¼ˆç« èŠ‚æ£€ç´¢ï¼‰

æµ‹è¯•ç›®æ ‡ï¼šç»™å®šé—®é¢˜å’Œç›®æ ‡è®ºæ–‡ï¼Œèƒ½å¦æ‰¾åˆ°æ­£ç¡®çš„ chunkï¼Ÿ

**ä½¿ç”¨çš„ RAG æ–¹æ³•**: `search_by_section(query, doc_id, section_category, k=10)`

| æŒ‡æ ‡ | å…¬å¼ | è¯´æ˜ |
|------|------|------|
| **Chunk Precision** | $P = \frac{\|Retrieved \cap Expected\|}{\|Retrieved\|}$ | è¿”å›çš„ chunks ä¸­æœ‰å¤šå°‘æ˜¯æœŸæœ›çš„ |
| **Chunk Recall** | $R = \frac{\|Retrieved \cap Expected\|}{\|Expected\|}$ | æœŸæœ›çš„ chunks ä¸­æœ‰å¤šå°‘è¢«è¿”å› |
| **Method Precision** | åŒä¸Šï¼Œé™å®š `section_category=2` | æ–¹æ³•è®ºç« èŠ‚çš„æ£€ç´¢ç²¾åº¦ |
| **Eval Precision** | åŒä¸Šï¼Œé™å®š `section_category=4` | å®éªŒç« èŠ‚çš„æ£€ç´¢ç²¾åº¦ |

**ä»£ç å®ç°**:
```python
# src/evaluation/runner.py

def run_l2_chunk_retrieval(self, qa_pairs: list[QAPair]) -> L2Result:
    for qa in valid_pairs:  # åªç”¨æœ‰ expected_chunk_ids çš„é—®é¢˜
        expected_chunks = set(qa.expected_chunk_ids)
        
        # æ ¹æ® answer_source ç¡®å®š section_category
        section_cat = self._source_to_category(qa.answer_source)
        
        # æ£€ç´¢
        results = self.rag.search_by_section(
            query=qa.question,
            doc_id=qa.expected_doc_ids[0],
            section_category=section_cat,
            k=10
        )
        
        retrieved_chunks = set(r["chunk_id"] for r in results)
        
        precision = len(expected_chunks & retrieved_chunks) / len(retrieved_chunks)
        recall = len(expected_chunks & retrieved_chunks) / len(expected_chunks)
```

---

### L3: End-to-End QAï¼ˆç«¯åˆ°ç«¯é—®ç­”ï¼‰

æµ‹è¯•ç›®æ ‡ï¼šå®Œæ•´çš„ RAG æµç¨‹èƒ½å¦ç”Ÿæˆæ­£ç¡®çš„ç­”æ¡ˆï¼Ÿ

**æµç¨‹**: æ£€ç´¢ â†’ LLM ç”Ÿæˆç­”æ¡ˆ â†’ LLM è¯„åˆ¤è´¨é‡

| æŒ‡æ ‡ | èŒƒå›´ | è¯´æ˜ |
|------|------|------|
| **Correctness** | 0-1 | ç­”æ¡ˆæ˜¯å¦æ­£ç¡®ï¼ˆä¸å‚è€ƒç­”æ¡ˆå¯¹æ¯”ï¼‰ |
| **Faithfulness** | 0-1 | ç­”æ¡ˆæ˜¯å¦å¿ å®äºæ£€ç´¢å†…å®¹ï¼ˆæ— å¹»è§‰ï¼‰ |
| **Relevance** | 0-1 | ç­”æ¡ˆæ˜¯å¦ç›´æ¥å›ç­”é—®é¢˜ |
| **Easy/Medium/Hard Accuracy** | 0-1 | æŒ‰éš¾åº¦åˆ†ç±»çš„æ­£ç¡®ç‡ |

**LLM-as-Judge è¯„åˆ†**:
```python
# src/evaluation/prompts/l3_evaluation.py

ANSWER_EVALUATION_PROMPT = """
è¯„åˆ†æ ‡å‡†ï¼š
1. Correctness (0-5): ç­”æ¡ˆæ˜¯å¦åŒ…å«æ­£ç¡®ä¿¡æ¯ï¼Ÿ
2. Faithfulness (0-5): ç­”æ¡ˆæ˜¯å¦åŸºäºæ£€ç´¢å†…å®¹ï¼Ÿ
3. Relevance (0-5): ç­”æ¡ˆæ˜¯å¦ç›´æ¥å›ç­”é—®é¢˜ï¼Ÿ

è¾“å‡º JSON: {"correctness": 4, "faithfulness": 5, "relevance": 4}
"""
```

**ä»£ç å®ç°**:
```python
# src/evaluation/runner.py

def run_l3_end_to_end(self, qa_pairs: list[QAPair]) -> L3Result:
    for qa in qa_pairs:
        # 1. æ£€ç´¢
        context = self._retrieve_context(qa)
        
        # 2. ç”Ÿæˆç­”æ¡ˆ
        generated_answer = self._generate_answer(qa.question, context)
        
        # 3. LLM è¯„åˆ¤
        scores = self._evaluate_answer(
            question=qa.question,
            generated_answer=generated_answer,
            reference_answer=qa.reference_answer,
            context=context
        )
        
        # å½’ä¸€åŒ–åˆ° 0-1
        correctness = scores["correctness"] / 5.0
        faithfulness = scores["faithfulness"] / 5.0
```

---

## å¯¹æ¯”å®éªŒé…ç½®

### 1. åˆ‡æ¢ Index ç±»å‹

æ”¯æŒçš„ç´¢å¼•ç±»å‹ï¼š`FLAT`, `HNSW`, `IVF_FLAT`

```python
from evaluation.config import EvaluationConfig, ChunkStrategy
from evaluation.data_preparation.collection_builder import CollectionBuilder

config = EvaluationConfig()
builder = CollectionBuilder(config)

# åˆ›å»ºä¸åŒç´¢å¼•ç±»å‹çš„ collection
builder.create_collection(
    strategy=ChunkStrategy.PARAGRAPH,
    index_type="HNSW",           # æˆ– "FLAT", "IVF_FLAT"
    drop_if_exists=True
)
```

**å®Œæ•´å¯¹æ¯”å®éªŒ**:
```python
from evaluation.runner import EvaluationRunner
from rag.milvus import MilvusProvider

results = {}
for index_type in ["FLAT", "HNSW", "IVF_FLAT"]:
    # 1. é‡å»º collection
    builder.create_collection(ChunkStrategy.PARAGRAPH, index_type=index_type)
    pipeline.rebuild_from_chunks(ChunkStrategy.PARAGRAPH)
    
    # 2. è¿è¡Œè¯„ä¼°
    with builder.use_chunk_strategy(ChunkStrategy.PARAGRAPH):
        milvus = MilvusProvider()
        runner = EvaluationRunner(rag_client=milvus, config=config)
        report = runner.run_all()
        results[index_type] = report
```

---

### 2. åˆ‡æ¢ Chunk ç­–ç•¥

æ”¯æŒçš„åˆ†å—ç­–ç•¥ï¼ˆå®šä¹‰åœ¨ `evaluation/config.py`ï¼‰:

| ç­–ç•¥ | è¯´æ˜ |
|------|------|
| `PARAGRAPH` | æŒ‰æ®µè½åˆ†å—ï¼ˆé»˜è®¤ï¼‰ |
| `SENTENCE` | æŒ‰å¥å­åˆ†å—ï¼ˆæ›´ç»†ç²’åº¦ï¼‰ |
| `CONTEXTUAL` | å¸¦ä¸Šä¸‹æ–‡å‰ç¼€çš„åˆ†å— |

```python
from evaluation.config import ChunkStrategy

# ä½¿ç”¨ä¸åŒç­–ç•¥
for strategy in [ChunkStrategy.PARAGRAPH, ChunkStrategy.SENTENCE]:
    with builder.use_chunk_strategy(strategy):
        milvus = MilvusProvider()
        runner = EvaluationRunner(rag_client=milvus)
        report = runner.run_all()
```

**æ³¨æ„**: ä¸åŒç­–ç•¥éœ€è¦åˆ†åˆ«å‡†å¤‡æ•°æ®ï¼š
```bash
# ä¸ºæ¯ç§ç­–ç•¥å‡†å¤‡ chunksï¼ˆä¼šåˆ›å»ºä¸åŒçš„ collectionï¼‰
uv run python scripts/run_full_evaluation.py --prepare-only --sample 20
```

---

### 3. åˆ‡æ¢ RAG ç±»å‹ï¼ˆAgentic vs Non-Agenticï¼‰

**Non-Agenticï¼ˆå½“å‰å®ç°ï¼‰**: ç›´æ¥å‘é‡æ£€ç´¢
```python
# ç›´æ¥è°ƒç”¨ MilvusProvider
results = milvus.search_abstracts(query, k=10)
```

**Agentic RAGï¼ˆæ‰©å±•ï¼‰**: éœ€è¦å®ç° AgenticSearcher
```python
# ä½¿ç”¨ Agent è¿›è¡Œå¤šæ­¥æ¨ç†æ£€ç´¢
from agents.searcher import AgenticSearcher

class AgenticRAG:
    def __init__(self, milvus: MilvusProvider, llm):
        self.milvus = milvus
        self.llm = llm
        self.searcher = AgenticSearcher(milvus, llm)
    
    def search_abstracts(self, query: str, k: int = 10):
        # Agent å†³å®šå¦‚ä½•æ£€ç´¢
        return self.searcher.search(query, k)
```

**å¯¹æ¯”æ–¹æ³•**:
```python
# Non-Agentic
runner_basic = EvaluationRunner(rag_client=milvus)

# Agentic  
agentic_rag = AgenticRAG(milvus, llm)
runner_agentic = EvaluationRunner(rag_client=agentic_rag)

# å¯¹æ¯”
report_basic = runner_basic.run_all()
report_agentic = runner_agentic.run_all()
```

---

### 4. å®Œæ•´ 12 é…ç½®å¯¹æ¯”å®éªŒ

```python
"""
12 = 2 (chunk) Ã— 3 (index) Ã— 2 (agentic)
"""

from itertools import product

chunk_strategies = [ChunkStrategy.PARAGRAPH, ChunkStrategy.SENTENCE]
index_types = ["FLAT", "HNSW", "IVF_FLAT"]
agentic_modes = [False, True]

all_results = []

for chunk, index, agentic in product(chunk_strategies, index_types, agentic_modes):
    config_name = f"{chunk.value}_{index}_{'agentic' if agentic else 'basic'}"
    
    # 1. å‡†å¤‡ collection
    builder.create_collection(chunk, index_type=index, drop_if_exists=True)
    pipeline.rebuild_from_chunks(chunk)
    
    # 2. é€‰æ‹© RAG å®¢æˆ·ç«¯
    with builder.use_chunk_strategy(chunk):
        milvus = MilvusProvider()
        rag_client = AgenticRAG(milvus, llm) if agentic else milvus
        
        # 3. è¿è¡Œè¯„ä¼°
        runner = EvaluationRunner(rag_client=rag_client, llm_client=llm)
        report = runner.run_all()
        
        all_results.append({
            "config": config_name,
            "l1_mrr": report.l1_paper_discovery.mrr,
            "l1_hit_rate": report.l1_paper_discovery.hit_rate,
            "l2_precision": report.l2_section_retrieval.overall_precision,
            "l3_accuracy": report.l3_end_to_end.overall_accuracy,
        })

# è¾“å‡ºå¯¹æ¯”è¡¨
import pandas as pd
df = pd.DataFrame(all_results)
print(df.to_markdown())
```

---

## æ•°æ®æµç¨‹

### ç›®å½•ç»“æ„

```
evaluation/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ papers_source.jsonl    # ä»ä¸šåŠ¡åº“å¯¼å‡ºçš„è®ºæ–‡å…ƒæ•°æ®
â”‚   â”œâ”€â”€ pdfs/                  # ä¸‹è½½çš„ PDF æ–‡ä»¶ï¼ˆç¼“å­˜ï¼‰
â”‚   â”œâ”€â”€ chunks/
â”‚   â”‚   â”œâ”€â”€ paragraph/         # PARAGRAPH ç­–ç•¥çš„ chunks
â”‚   â”‚   â”‚   â”œâ”€â”€ {doc_id}.json
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â””â”€â”€ sentence/          # SENTENCE ç­–ç•¥çš„ chunks
â”‚   â”œâ”€â”€ ground_truth.json      # ç”Ÿæˆçš„ QA pairs
â”‚   â””â”€â”€ reports/
â”‚       â””â”€â”€ report_{id}_{timestamp}.json
```

### Chunk æ–‡ä»¶æ ¼å¼

```json
{
  "doc_id": "db31bfbb-b56a-4547-8c2f-5ae2b2466c52",
  "title": "Security at the End of the Tunnel...",
  "strategy": "paragraph",
  "chunks": [
    {
      "chunk_index": 0,
      "chunk_text": "We present a qualitative study...",
      "section_title": "Abstract",
      "section_category": 0,
      "contextual_prefix": "This paper discusses VPN security..."
    }
  ]
}
```

### Ground Truth æ ¼å¼

```json
{
  "version": "1.0",
  "created_at": "2025-11-26T21:29:18",
  "total_papers": 8,
  "difficulty_distribution": {"easy": 2, "medium": 2, "hard": 1},
  "qa_pairs": [
    {
      "id": 1,
      "question": "Which paper discusses corporate VPN mental models?",
      "difficulty": "easy",
      "expected_doc_ids": ["db31bfbb-..."],
      "expected_chunk_ids": null,
      "answer_source": "abstract",
      "reference_answer": "The paper 'Security at the End...' discusses..."
    }
  ]
}
```

### è¯„ä¼°æŠ¥å‘Šæ ¼å¼

```json
{
  "run_id": "7c015e51",
  "run_at": "2025-11-27T11:00:21",
  "total_qa_pairs": 5,
  "rag_provider": "MilvusProvider",
  "embedding_model": "qwen3-embedding:4b",
  "l1_paper_discovery": {
    "precision_at_5": 0.24,
    "precision_at_10": 0.12,
    "recall_at_5": 1.0,
    "recall_at_10": 1.0,
    "mrr": 1.0,
    "hit_rate": 1.0,
    "mean_latency_ms": 695.1
  },
  "l2_section_retrieval": {
    "overall_precision": 0.25,
    "overall_recall": 0.5,
    "method_precision": 0.25
  },
  "l3_end_to_end": {
    "easy_accuracy": 1.0,
    "medium_accuracy": 0.6,
    "hard_accuracy": 1.0,
    "overall_accuracy": 0.84,
    "faithfulness": 1.0
  }
}
```

---

## API å‚è€ƒ

### EvaluationRunner

```python
class EvaluationRunner:
    def __init__(
        self,
        rag_client: RAG,                    # MilvusProvider æˆ–å…¼å®¹æ¥å£
        llm_client: BaseChatModel = None,   # L3 è¯„ä¼°éœ€è¦
        config: EvaluationConfig = None
    ):
        ...
    
    def run_all(self, ground_truth: GroundTruth = None) -> EvaluationReport:
        """è¿è¡Œå®Œæ•´è¯„ä¼°ï¼ˆL1 + L2 + L3ï¼‰"""
    
    def run_l1_paper_discovery(self, qa_pairs: list[QAPair]) -> L1Result:
        """L1: è®ºæ–‡å‘ç°è¯„ä¼°"""
    
    def run_l2_chunk_retrieval(self, qa_pairs: list[QAPair]) -> L2Result:
        """L2: Chunk æ£€ç´¢è¯„ä¼°"""
    
    def run_l3_end_to_end(self, qa_pairs: list[QAPair]) -> L3Result:
        """L3: ç«¯åˆ°ç«¯ QA è¯„ä¼°"""
    
    def load_ground_truth(self) -> GroundTruth:
        """åŠ è½½ Ground Truth"""
    
    def save_report(self, report: EvaluationReport) -> Path:
        """ä¿å­˜è¯„ä¼°æŠ¥å‘Š"""
```

### CollectionBuilder

```python
class CollectionBuilder:
    def create_collection(
        self,
        strategy: ChunkStrategy,
        index_type: str = "FLAT",
        drop_if_exists: bool = False
    ):
        """åˆ›å»ºè¯„ä¼° collection"""
    
    @contextmanager
    def use_chunk_strategy(self, strategy: ChunkStrategy):
        """Context Manager: ä¸´æ—¶åˆ‡æ¢åˆ°æŒ‡å®šç­–ç•¥çš„ collection"""
        # ç”¨æ³•:
        # with builder.use_chunk_strategy(ChunkStrategy.PARAGRAPH):
        #     milvus = MilvusProvider()  # ä½¿ç”¨åˆ‡æ¢åçš„ collection
```

### QAGenerator

```python
class QAGenerator:
    def generate(
        self,
        strategy: ChunkStrategy = ChunkStrategy.PARAGRAPH,
        num_questions: int = 50,
        difficulty_distribution: dict = {"easy": 0.4, "medium": 0.4, "hard": 0.2}
    ) -> GroundTruth:
        """ç”Ÿæˆ QA pairs"""
    
    def save_ground_truth(self, ground_truth: GroundTruth) -> Path:
        """ä¿å­˜åˆ°æ–‡ä»¶"""
```

### DataPreparationPipeline

```python
class DataPreparationPipeline:
    def run(
        self,
        strategies: list[ChunkStrategy] = None,
        sample_size: int = None,
        drop_existing: bool = False
    ) -> PipelineResult:
        """è¿è¡Œå®Œæ•´æ•°æ®å‡†å¤‡æµç¨‹"""
    
    def rebuild_from_chunks(
        self,
        strategy: ChunkStrategy,
        drop_existing: bool = True
    ) -> int:
        """ä»å·²æœ‰ chunks æ–‡ä»¶é‡å»º collectionï¼ˆå¿«é€Ÿï¼‰"""
```

---

## æœ€ä½³å®è·µ

### 1. å¿«é€Ÿè¿­ä»£æµ‹è¯•

```bash
# ä½¿ç”¨å°æ ·æœ¬å¿«é€ŸéªŒè¯
uv run python scripts/run_full_evaluation.py --full --sample 10 --num-questions 15 --no-l3
```

### 2. åªæ›´æ–° QA ä¸é‡æ–°å¤„ç† PDF

```bash
# ä½¿ç”¨å·²æœ‰ chunks é‡æ–°ç”Ÿæˆé—®é¢˜
uv run python scripts/run_full_evaluation.py --generate-qa --num-questions 50
```

### 3. å¯¹æ¯”ä¸åŒç´¢å¼•æ•ˆæœ

```python
# ä½¿ç”¨ rebuild_from_chunks å¿«é€Ÿåˆ‡æ¢ç´¢å¼•
for index_type in ["FLAT", "HNSW"]:
    builder.create_collection(ChunkStrategy.PARAGRAPH, index_type=index_type)
    pipeline.rebuild_from_chunks(ChunkStrategy.PARAGRAPH)
    # è¿è¡Œè¯„ä¼°...
```

### 4. æŸ¥çœ‹å†å²æŠ¥å‘Š

```bash
ls evaluation/data/reports/
cat evaluation/data/reports/report_xxx.json | jq .
```

---

## å¸¸è§é—®é¢˜

### Q: L3 è¯„ä¼°ä¸ºä»€ä¹ˆè¿™ä¹ˆæ…¢ï¼Ÿ
A: L3 éœ€è¦å¯¹æ¯ä¸ªé—®é¢˜è°ƒç”¨ä¸¤æ¬¡ LLMï¼ˆç”Ÿæˆç­”æ¡ˆ + è¯„åˆ¤è´¨é‡ï¼‰ã€‚å¯ä»¥ç”¨ `--no-l3` è·³è¿‡ã€‚

### Q: å¦‚ä½•å¢åŠ æµ‹è¯•é—®é¢˜æ•°é‡ï¼Ÿ
A: è¿è¡Œ `--generate-qa --num-questions 100`ï¼Œä¼šè¦†ç›–ç°æœ‰çš„ ground_truth.jsonã€‚

### Q: è¯„ä¼° collection å’Œä¸šåŠ¡ collection ä¼šå†²çªå—ï¼Ÿ
A: ä¸ä¼šã€‚è¯„ä¼°ä½¿ç”¨ç‹¬ç«‹çš„ collectionï¼ˆå¦‚ `papers_eval_paragraph`ï¼‰ï¼Œé€šè¿‡ `CollectionBuilder.use_chunk_strategy()` ä¸´æ—¶åˆ‡æ¢ã€‚

### Q: å¦‚ä½•æ·»åŠ æ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼Ÿ
A: åœ¨ `src/evaluation/schemas.py` æ·»åŠ å­—æ®µï¼Œåœ¨ `src/evaluation/runner.py` å®ç°è®¡ç®—é€»è¾‘ã€‚
