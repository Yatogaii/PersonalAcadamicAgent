"""
å®Œæ•´è¯„ä¼°æµç¨‹è„šæœ¬

ä»ä¸šåŠ¡ collection å¯¼å‡ºæ•°æ® â†’ ç”Ÿæˆ QA â†’ è¿è¡Œè¯„ä¼°

ä½¿ç”¨æ–¹å¼:
    # å®Œæ•´å¯¹æ¯”å®éªŒï¼ˆ12ç§é…ç½®: 2 chunk Ã— 3 index Ã— 2 agenticï¼‰
    uv run python scripts/run_full_evaluation.py --compare --sample 20 --num-questions 30
    
    # å¿«é€Ÿå¯¹æ¯”ï¼ˆè·³è¿‡ L3ï¼Œåªæ¯”è¾ƒæ£€ç´¢æ€§èƒ½ï¼‰
    uv run python scripts/run_full_evaluation.py --compare --sample 20 --no-l3
    
    # å•é…ç½®è¯„ä¼°ï¼ˆä½¿ç”¨å·²æœ‰æ•°æ®ï¼‰
    uv run python scripts/run_full_evaluation.py --eval-only
    
    # ç”Ÿæˆæ–°çš„ QAï¼ˆä½¿ç”¨å·²æœ‰ chunksï¼‰
    uv run python scripts/run_full_evaluation.py --generate-qa --num-questions 20
    
    # æŸ¥çœ‹å½“å‰æ•°æ®çŠ¶æ€
    uv run python scripts/run_full_evaluation.py --status
    
    # æ—§çš„ --full æ¨¡å¼ï¼ˆåªè·‘å•é…ç½®ï¼‰
    uv run python scripts/run_full_evaluation.py --full --sample 20
"""

import sys
import argparse
from pathlib import Path

# æ·»åŠ  src åˆ° path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from logging_config import logger


def step1_export_and_prepare(
    sample_size: int = None, 
    drop_existing: bool = False,
    all_strategies: bool = False
):
    """
    Step 1: ä»ä¸šåŠ¡åº“å¯¼å‡ºæ•°æ®å¹¶å‡†å¤‡è¯„ä¼° collection
    
    Args:
        sample_size: æŠ½æ ·æ•°é‡
        drop_existing: æ˜¯å¦åˆ é™¤å·²æœ‰ collection
        all_strategies: æ˜¯å¦å‡†å¤‡æ‰€æœ‰ chunk ç­–ç•¥ (paragraph + contextual)
    """
    from rag.milvus import MilvusProvider
    from models import get_llm_by_usage
    from evaluation.config import EvaluationConfig, ChunkStrategy
    from evaluation.data_preparation.pipeline import DataPreparationPipeline
    
    print("=" * 70)
    print("Step 1: æ•°æ®å‡†å¤‡ (Export & Prepare)")
    print("=" * 70)
    
    # åˆå§‹åŒ–
    config = EvaluationConfig()
    source_rag = MilvusProvider()
    llm = get_llm_by_usage('evaluation')
    
    print(f"æº Collection: {source_rag.collection}")
    print(f"è¯„ä¼°æ•°æ®ç›®å½•: {config.data_dir}")
    
    # åˆ›å»º pipeline
    pipeline = DataPreparationPipeline(
        source_rag_client=source_rag,
        llm_client=llm,
        config=config
    )
    
    # é€‰æ‹©ç­–ç•¥
    if all_strategies:
        strategies = [ChunkStrategy.PARAGRAPH, ChunkStrategy.CONTEXTUAL]
        print(f"ç­–ç•¥: paragraph + contextual (ä¸¤ä¸ª collection)")
    else:
        strategies = [ChunkStrategy.PARAGRAPH]
        print(f"ç­–ç•¥: paragraph only")
    
    # è¿è¡Œ
    result = pipeline.run(
        strategies=strategies,
        sample_size=sample_size,
        drop_existing=drop_existing
    )
    
    print(f"\nâœ“ å¯¼å‡ºè®ºæ–‡: {result.papers_exported}")
    for strategy in strategies:
        s = strategy.value
        print(f"âœ“ {s}: å¤„ç† {result.papers_success.get(s, 0)} ç¯‡, chunks {result.chunks_saved.get(s, 0)} ä¸ª")
    
    return result


def step2_generate_qa(num_questions: int = 50):
    """
    Step 2: ä» chunks ç”Ÿæˆ QA pairs
    """
    from models import get_llm_by_usage
    from evaluation.config import EvaluationConfig, ChunkStrategy
    from evaluation.qa_generation.qa_generator import QAGenerator
    
    print("\n" + "=" * 70)
    print("Step 2: QA ç”Ÿæˆ")
    print("=" * 70)
    
    config = EvaluationConfig()
    llm = get_llm_by_usage('evaluation')
    
    generator = QAGenerator(llm_client=llm, config=config)
    
    # æ£€æŸ¥ chunks æ˜¯å¦å­˜åœ¨
    chunks_dir = config.chunks_dir / "paragraph"
    if not chunks_dir.exists() or not list(chunks_dir.glob("*.json")):
        print("âš  æ²¡æœ‰æ‰¾åˆ° chunks æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ step1")
        return None
    
    print(f"Chunks ç›®å½•: {chunks_dir}")
    print(f"ç”Ÿæˆ {num_questions} ä¸ªé—®é¢˜...")
    
    # ç”Ÿæˆ QAï¼ˆä½¿ç”¨æ–°çš„ 4 çº§éš¾åº¦åˆ†å¸ƒï¼‰
    ground_truth = generator.generate(
        strategy=ChunkStrategy.PARAGRAPH,
        num_questions=num_questions,
        difficulty_distribution={
            "easy": 0.2,      # Level 1: å•è®ºæ–‡ç²¾ç¡®é¢˜
            "medium": 0.3,    # Level 2: å•è®ºæ–‡æ¨ç†é¢˜
            "hard": 0.3,      # Level 3: è·¨è®ºæ–‡æ¯”è¾ƒé¢˜
            "expert": 0.2     # Level 4: é¢†åŸŸç»¼è¿°é¢˜
        }
    )
    
    # ä¿å­˜
    save_path = generator.save(ground_truth)
    
    print(f"\nâœ“ ç”Ÿæˆ {len(ground_truth.qa_pairs)} ä¸ªé—®é¢˜")
    print(f"âœ“ ä¿å­˜åˆ°: {save_path}")
    print(f"  - Level 1 (Easy): {ground_truth.difficulty_distribution.get('easy', 0)}")
    print(f"  - Level 2 (Medium): {ground_truth.difficulty_distribution.get('medium', 0)}")
    print(f"  - Level 3 (Hard): {ground_truth.difficulty_distribution.get('hard', 0)}")
    
    return ground_truth


def step3_run_evaluation(run_l3: bool = True):
    """
    Step 3: è¿è¡Œè¯„ä¼°
    """
    from models import get_llm_by_usage
    from rag.milvus import MilvusProvider
    from evaluation.config import EvaluationConfig, ChunkStrategy
    from evaluation.runner import EvaluationRunner
    from evaluation.data_preparation.collection_builder import CollectionBuilder
    
    print("\n" + "=" * 70)
    print("Step 3: è¿è¡Œè¯„ä¼°")
    print("=" * 70)
    
    config = EvaluationConfig()
    
    # æ£€æŸ¥ ground truth
    if not config.ground_truth_file.exists():
        print("âš  æ²¡æœ‰æ‰¾åˆ° ground_truth.jsonï¼Œè¯·å…ˆè¿è¡Œ step2")
        return None
    
    builder = CollectionBuilder(config)
    llm = get_llm_by_usage('evaluation') if run_l3 else None
    
    print(f"Ground Truth: {config.ground_truth_file}")
    print(f"è¯„ä¼° Collection: papers_eval_paragraph")
    print(f"L3 è¯„ä¼°: {'å¯ç”¨' if run_l3 else 'ç¦ç”¨'}")
    
    with builder.use_chunk_strategy(ChunkStrategy.PARAGRAPH):
        milvus = MilvusProvider()
        
        runner = EvaluationRunner(
            rag_client=milvus,
            llm_client=llm,
            config=config
        )
        
        # åŠ è½½ ground truth
        ground_truth = runner.load_ground_truth()
        print(f"\nåŠ è½½ {len(ground_truth.qa_pairs)} ä¸ªæµ‹è¯•é—®é¢˜")
        
        # è¿è¡Œè¯„ä¼°
        print("\nå¼€å§‹è¯„ä¼°...")
        report = runner.run_all(ground_truth)
        
        # æ‰“å°ç»“æœ
        runner.print_report(report)
        
        # ä¿å­˜æŠ¥å‘Š
        save_path = runner.save_report(report)
        print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_path}")
    
    return report


def show_status():
    """æ˜¾ç¤ºå½“å‰æ•°æ®çŠ¶æ€"""
    from evaluation.config import EvaluationConfig
    import json
    
    print("=" * 70)
    print("è¯„ä¼°æ•°æ®çŠ¶æ€")
    print("=" * 70)
    
    config = EvaluationConfig()
    
    # 1. æ£€æŸ¥ source papers
    if config.source_file.exists():
        with open(config.source_file, "r") as f:
            papers_count = sum(1 for _ in f)
        print(f"\nğŸ“„ Source Papers: {config.source_file}")
        print(f"   è®ºæ–‡æ•°é‡: {papers_count}")
    else:
        print(f"\nğŸ“„ Source Papers: ä¸å­˜åœ¨")
    
    # 2. æ£€æŸ¥ chunks
    chunks_dir = config.chunks_dir / "paragraph"
    if chunks_dir.exists():
        chunk_files = list(chunks_dir.glob("*.json"))
        total_chunks = 0
        for f in chunk_files:
            with open(f, "r") as file:
                data = json.load(file)
                total_chunks += len(data.get("chunks", []))
        print(f"\nğŸ“¦ Chunks (paragraph): {chunks_dir}")
        print(f"   è®ºæ–‡æ•°: {len(chunk_files)}")
        print(f"   æ€» chunks: {total_chunks}")
    else:
        print(f"\nğŸ“¦ Chunks: ä¸å­˜åœ¨")
    
    # 3. æ£€æŸ¥ ground truth
    if config.ground_truth_file.exists():
        with open(config.ground_truth_file, "r") as f:
            gt = json.load(f)
        qa_pairs = gt.get("qa_pairs", [])
        print(f"\nâ“ Ground Truth: {config.ground_truth_file}")
        print(f"   QA æ•°é‡: {len(qa_pairs)}")
        print(f"   éš¾åº¦åˆ†å¸ƒ: {gt.get('difficulty_distribution', {})}")
    else:
        print(f"\nâ“ Ground Truth: ä¸å­˜åœ¨")
    
    # 4. æ£€æŸ¥è¯„ä¼° collection
    try:
        from rag.milvus import MilvusProvider
        from evaluation.data_preparation.collection_builder import CollectionBuilder
        from evaluation.config import ChunkStrategy
        
        builder = CollectionBuilder(config)
        stats = builder.get_collection_stats(ChunkStrategy.PARAGRAPH)
        if stats:
            print(f"\nğŸ—„ï¸ Eval Collection: {stats.name}")
            print(f"   æ€»è®°å½•: {stats.total_records}")
            print(f"   è®ºæ–‡æ•°: {stats.total_papers}")
            print(f"   ç´¢å¼•ç±»å‹: {stats.index_type}")
        else:
            print(f"\nğŸ—„ï¸ Eval Collection: ä¸å­˜åœ¨æˆ–æœªåˆå§‹åŒ–")
    except Exception as e:
        print(f"\nğŸ—„ï¸ Eval Collection: æ£€æŸ¥å¤±è´¥ ({e})")
    
    # 5. æ£€æŸ¥æŠ¥å‘Š
    reports_dir = config.reports_dir
    if reports_dir.exists():
        reports = list(reports_dir.glob("report_*.json"))
        print(f"\nğŸ“Š Reports: {reports_dir}")
        print(f"   æŠ¥å‘Šæ•°é‡: {len(reports)}")
        if reports:
            # æ˜¾ç¤ºæœ€è¿‘çš„æŠ¥å‘Š
            latest = max(reports, key=lambda x: x.stat().st_mtime)
            print(f"   æœ€æ–°æŠ¥å‘Š: {latest.name}")
    else:
        print(f"\nğŸ“Š Reports: ä¸å­˜åœ¨")
    
    print("\n" + "=" * 70)


def run_full_pipeline(sample_size: int = None, num_questions: int = 50, run_l3: bool = True):
    """è¿è¡Œå®Œæ•´æµç¨‹"""
    print("\n" + "=" * 70)
    print("å®Œæ•´è¯„ä¼°æµç¨‹")
    print("=" * 70)
    print(f"  æ ·æœ¬å¤§å°: {sample_size if sample_size else 'å…¨é‡'}")
    print(f"  é—®é¢˜æ•°é‡: {num_questions}")
    print(f"  L3 è¯„ä¼°: {'å¯ç”¨' if run_l3 else 'ç¦ç”¨'}")
    print("=" * 70)
    
    # Step 1: æ•°æ®å‡†å¤‡
    step1_export_and_prepare(sample_size=sample_size, drop_existing=True)
    
    # Step 2: ç”Ÿæˆ QA
    step2_generate_qa(num_questions=num_questions)
    
    # Step 3: è¿è¡Œè¯„ä¼°
    report = step3_run_evaluation(run_l3=run_l3)
    
    print("\n" + "=" * 70)
    print("âœ“ è¯„ä¼°æµç¨‹å®Œæˆ!")
    print("=" * 70)
    
    return report


def run_comparison(
    sample_size: int = None,
    num_questions: int = 50,
    run_l3: bool = True,
    skip_data_preparation: bool = False,
    resume: bool = True,
    clear_cache: bool = False
):
    """
    è¿è¡Œå®Œæ•´å¯¹æ¯”å®éªŒ
    
    12 ç§é…ç½® = 2 (chunk) Ã— 3 (index) Ã— 2 (agentic)
    
    Args:
        resume: æ˜¯å¦ä»ç¼“å­˜æ¢å¤ï¼ˆè·³è¿‡å·²å®Œæˆçš„å®éªŒï¼‰
        clear_cache: æ¸…é™¤ç¼“å­˜åé‡æ–°è¿è¡Œ
    """
    from models import get_llm_by_usage
    from evaluation.config import EvaluationConfig
    from evaluation.comparison_runner import ComparisonRunner
    
    print("\n" + "=" * 70)
    print("å®Œæ•´å¯¹æ¯”å®éªŒ")
    print("=" * 70)
    
    config = EvaluationConfig()
    all_experiments = config.get_all_experiments()
    
    print(f"  å®éªŒé…ç½®æ•°: {len(all_experiments)}")
    print(f"  Chunk ç­–ç•¥: {[c.value for c in config.chunk_strategies]}")
    print(f"  Index ç±»å‹: {[i.value for i in config.index_types]}")
    print(f"  Agentic æ¨¡å¼: [False, True]")
    print(f"  æ ·æœ¬å¤§å°: {sample_size if sample_size else 'å…¨é‡'}")
    print(f"  é—®é¢˜æ•°é‡: {num_questions}")
    print(f"  L3 è¯„ä¼°: {'å¯ç”¨' if run_l3 else 'ç¦ç”¨'}")
    print(f"  æ–­ç‚¹ç»­è·‘: {'å¯ç”¨' if resume else 'ç¦ç”¨'}")
    print("=" * 70)
    
    # åˆå§‹åŒ– LLM
    llm = get_llm_by_usage('evaluation')
    
    # åˆ›å»ºå¯¹æ¯”è¿è¡Œå™¨
    runner = ComparisonRunner(llm_client=llm, config=config)
    
    # æ¸…é™¤ç¼“å­˜ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if clear_cache:
        print("\nâš ï¸  æ¸…é™¤å®éªŒç¼“å­˜...")
        runner.clear_cache()
    
    # Step 1: æ•°æ®å‡†å¤‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if not skip_data_preparation:
        print("\n" + "-" * 70)
        print("Step 1: æ•°æ®å‡†å¤‡")
        print("-" * 70)
        runner.prepare_data(sample_size=sample_size)
    
    # Step 2: ç”Ÿæˆ QAï¼ˆå¦‚æœéœ€è¦ï¼‰
    if not config.ground_truth_file.exists():
        print("\n" + "-" * 70)
        print("Step 2: ç”Ÿæˆ QA")
        print("-" * 70)
        step2_generate_qa(num_questions=num_questions)
    
    # Step 3: è¿è¡Œæ‰€æœ‰å®éªŒ
    print("\n" + "-" * 70)
    print("Step 3: è¿è¡Œå¯¹æ¯”å®éªŒ")
    print("-" * 70)
    
    comparison = runner.run_all_experiments(
        run_l3=run_l3,
        skip_data_preparation=True,  # å·²ç»åœ¨ä¸Šé¢å‡†å¤‡å¥½äº†
        resume=resume
    )
    
    # æ‰“å°å’Œä¿å­˜ç»“æœ
    runner.print_comparison(comparison)
    save_path = runner.save_comparison(comparison)
    
    print("\n" + "=" * 70)
    print("âœ“ å¯¹æ¯”å®éªŒå®Œæˆ!")
    print(f"  æŠ¥å‘Š: {save_path}")
    print(f"  Markdown: {save_path.with_suffix('.md')}")
    print("=" * 70)
    
    return comparison


def main():
    parser = argparse.ArgumentParser(description="å®Œæ•´è¯„ä¼°æµç¨‹")
    
    # è¿è¡Œæ¨¡å¼
    parser.add_argument("--compare", action="store_true",
                       help="è¿è¡Œå®Œæ•´å¯¹æ¯”å®éªŒ (12ç§é…ç½®)")
    parser.add_argument("--full", action="store_true", 
                       help="è¿è¡Œå•é…ç½®å®Œæ•´æµç¨‹ (export â†’ qa â†’ eval)")
    parser.add_argument("--prepare-only", action="store_true",
                       help="åªè¿è¡Œ Step 1: æ•°æ®å‡†å¤‡")
    parser.add_argument("--generate-qa", action="store_true",
                       help="åªè¿è¡Œ Step 2: QA ç”Ÿæˆ")
    parser.add_argument("--eval-only", action="store_true",
                       help="åªè¿è¡Œ Step 3: è¯„ä¼°")
    parser.add_argument("--status", action="store_true",
                       help="æ˜¾ç¤ºå½“å‰æ•°æ®çŠ¶æ€")
    
    # å‚æ•°
    parser.add_argument("--sample", type=int, default=None,
                       help="æŠ½æ ·æ•°é‡ (é»˜è®¤å…¨é‡)")
    parser.add_argument("--num-questions", type=int, default=50,
                       help="ç”Ÿæˆé—®é¢˜æ•°é‡ (é»˜è®¤ 50)")
    parser.add_argument("--no-l3", action="store_true",
                       help="è·³è¿‡ L3 è¯„ä¼° (æ›´å¿«)")
    parser.add_argument("--drop-existing", action="store_true",
                       help="åˆ é™¤å·²æœ‰çš„è¯„ä¼° collection")
    parser.add_argument("--skip-data-prep", action="store_true",
                       help="è·³è¿‡æ•°æ®å‡†å¤‡ï¼ˆä½¿ç”¨å·²æœ‰æ•°æ®ï¼‰")
    parser.add_argument("--no-resume", action="store_true",
                       help="ä¸ä½¿ç”¨ç¼“å­˜ï¼Œä»å¤´è¿è¡Œæ‰€æœ‰å®éªŒ")
    parser.add_argument("--clear-cache", action="store_true",
                       help="æ¸…é™¤å®éªŒç¼“å­˜åè¿è¡Œ")
    parser.add_argument("--all-strategies", action="store_true",
                       help="å‡†å¤‡æ‰€æœ‰ chunk ç­–ç•¥ (paragraph + contextual)")
    
    args = parser.parse_args()
    
    run_l3 = not args.no_l3
    
    if args.status:
        show_status()
    elif args.compare:
        run_comparison(
            sample_size=args.sample,
            num_questions=args.num_questions,
            run_l3=run_l3,
            skip_data_preparation=args.skip_data_prep,
            resume=not args.no_resume,
            clear_cache=args.clear_cache
        )
    elif args.full:
        run_full_pipeline(
            sample_size=args.sample,
            num_questions=args.num_questions,
            run_l3=run_l3
        )
    elif args.prepare_only:
        step1_export_and_prepare(
            sample_size=args.sample,
            drop_existing=args.drop_existing,
            all_strategies=args.all_strategies
        )
    elif args.generate_qa:
        step2_generate_qa(num_questions=args.num_questions)
    elif args.eval_only:
        step3_run_evaluation(run_l3=run_l3)
    else:
        # é»˜è®¤æ˜¾ç¤ºå¸®åŠ©
        parser.print_help()


if __name__ == "__main__":
    main()
